{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# مساعد - متحدثك الذكي"
      ],
      "metadata": {
        "id": "L620hr-tIIet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TTS and STT"
      ],
      "metadata": {
        "id": "UV8VMt-cSjy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A comparison of various Speech-to-Text (STT) tools is conducted in the following notebook: [here](https://colab.research.google.com/drive/1pC9WBNFB361Fp_GBAeEytwPO0y5eD7FW?usp=sharing)"
      ],
      "metadata": {
        "id": "tQddOi0hdjdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can use whisper https://github.com/quic/ai-hub-apps/tree/main/apps/windows/python/Whisper"
      ],
      "metadata": {
        "id": "Yu1VRaUUakGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install openai-whisper"
      ],
      "metadata": {
        "id": "2EX2I3RHZ8cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d320179-6148-4824-d0cf-cd9458ee35a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "OFkGDr9crN7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71cbc1b-dbc1-49a4-dbc6-4f7e1e46ef5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 41.2MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# result1 = model.transcribe(\"I'm Sick.wav\")\n",
        "result2 = model.transcribe(\"Request.wav\",  language=\"ar\")\n"
      ],
      "metadata": {
        "id": "oOxq8svuDkxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the LLM"
      ],
      "metadata": {
        "id": "cp3F4ccdIWFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing The Required Libraries"
      ],
      "metadata": {
        "id": "UlhGJwTbIOFt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82UIsM2p_wYO"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate sentencepiece einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"humain-ai/ALLaM-7B-Instruct-preview\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def generate_allam(prompt, max_new_tokens=256):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "s_XTER8YG6er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompots Templates (three prompt for three tasks)"
      ],
      "metadata": {
        "id": "mKHBtZ8ATMQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "INTENT_SYSTEM_PROMPT = \"\"\"\n",
        "انت مساعد ذكي لنظام خدمات حكومية.\n",
        "مهمتك فقط تصنيف نية المستخدم من الرسالة.\n",
        "الأنواع المحتملة:\n",
        "- شرح خدمة\n",
        "- بدء خدمة جديدة\n",
        "- استكمال خدمة قائمة\n",
        "- استفسار عام\n",
        "- مشكلة تقنية في الدخول\n",
        "\n",
        "ارجع الناتج ككائن JSON فقط بهذا الشكل:\n",
        "{\"intent\": \"<اكتب النوع هنا بالضبط من القائمة>\"}\n",
        "\"\"\"\n",
        "\n",
        "def build_intent_prompt(user_message):\n",
        "    return INTENT_SYSTEM_PROMPT + \"\\n\\nرسالة المستخدم:\\n\" + user_message\n"
      ],
      "metadata": {
        "id": "zIAX7tLTHC2k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_intent(user_message):\n",
        "    prompt = build_intent_prompt(user_message)\n",
        "    raw = generate_allam(prompt, max_new_tokens=64)\n",
        "    # محاولة استخراج JSON من المخرجات\n",
        "    start = raw.find(\"{\")\n",
        "    end = raw.rfind(\"}\")\n",
        "    if start != -1 and end != -1:\n",
        "        json_str = raw[start:end+1]\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            return {\"intent\": None, \"raw\": raw}\n",
        "    return {\"intent\": None, \"raw\": raw}\n"
      ],
      "metadata": {
        "id": "HeLLL0V4HahX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SLOT_SYSTEM_PROMPT = \"\"\"\n",
        "انت مساعد استخراج حقول لمحادثة خدمات حكومية.\n",
        "يُعطى لك نص المستخدم وقائمة بالحقول المتوقعة.\n",
        "مهمتك:\n",
        "- استنتاج قيمة كل حقل من النص اذا كانت موجودة\n",
        "- اذا لم توجد القيمة اجعلها null\n",
        "\n",
        "ارجع الناتج ككائن JSON بالشكل:\n",
        "{\"slots\": {\"field1\": \"value or null\", \"field2\": \"value or null\", ...}}\n",
        "\"\"\"\n",
        "\n",
        "def build_slot_prompt(user_message, expected_slots):\n",
        "    return (\n",
        "        SLOT_SYSTEM_PROMPT\n",
        "        + \"\\n\\nالحقول المطلوبة:\\n\"\n",
        "        + \", \".join(expected_slots)\n",
        "        + \"\\n\\nرسالة المستخدم:\\n\"\n",
        "        + user_message\n",
        "    )\n",
        "\n",
        "def extract_slots(user_message, expected_slots):\n",
        "    prompt = build_slot_prompt(user_message, expected_slots)\n",
        "    raw = generate_allam(prompt, max_new_tokens=128)\n",
        "    start = raw.find(\"{\")\n",
        "    end = raw.rfind(\"}\")\n",
        "    if start != -1 and end != -1:\n",
        "        json_str = raw[start:end+1]\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            return {\"slots\": {}, \"raw\": raw}\n",
        "    return {\"slots\": {}, \"raw\": raw}\n"
      ],
      "metadata": {
        "id": "kJsoX9b6HfVo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_SYSTEM_PROMPT = \"\"\"\n",
        "انت مساعد رسمي يشرح خدمات نظام حكومي.\n",
        "لديك مقاطع نصية رسمية من دليل الخدمات.\n",
        "استخدم المعلومات الموجودة فقط في هذه المقاطع للاجابة.\n",
        "لا تضف معلومات من عندك.\n",
        "\n",
        "اكتب الاجابة بالعربية الواضحة في فقرتين كحد اقصى.\n",
        "\"\"\"\n",
        "\n",
        "def build_rag_prompt(user_message, context_chunks):\n",
        "    context_text = \"\\n\\n--- مقاطع من الدليل الرسمي ---\\n\" + \"\\n\\n\".join(context_chunks)\n",
        "    return RAG_SYSTEM_PROMPT + context_text + \"\\n\\nسؤال المستخدم:\\n\" + user_message\n",
        "\n",
        "def rag_answer(user_message, context_chunks):\n",
        "    prompt = build_rag_prompt(user_message, context_chunks)\n",
        "    answer = generate_allam(prompt, max_new_tokens=256)\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "wDXqI4BhHgHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STEP_GUIDE_PROMPT = \"\"\"\n",
        "انت مساعد حواري تنفذ خدمة حكومية خطوة بخطوة.\n",
        "يعطى لك:\n",
        "- وصف مختصر لمراحل الخدمة\n",
        "- حالة جلسة المستخدم الحالية\n",
        "- قائمة بالحقول الناقصة\n",
        "\n",
        "مهمتك:\n",
        "- كتابة رسالة قصيرة للمستخدم تسأله عن الحقول الناقصة\n",
        "- او تشرح له الخطوة التالية بلغة بسيطة\n",
        "- لا تذكر تفاصيل تقنية او JSON في الرسالة للمستخدم\n",
        "\"\"\"\n",
        "\n",
        "def build_step_prompt(service_schema, session_state, missing_fields):\n",
        "    return (\n",
        "        STEP_GUIDE_PROMPT\n",
        "        + \"\\n\\nمخطط الخدمة:\\n\"\n",
        "        + json.dumps(service_schema, ensure_ascii=False)\n",
        "        + \"\\n\\nحالة الجلسة:\\n\"\n",
        "        + json.dumps(session_state, ensure_ascii=False)\n",
        "        + \"\\n\\nالحقول الناقصة:\\n\"\n",
        "        + \", \".join(missing_fields)\n",
        "        + \"\\n\\nاكتب رسالة موجهة للمستخدم:\"\n",
        "    )\n",
        "\n",
        "def step_guide(service_schema, session_state, missing_fields):\n",
        "    prompt = build_step_prompt(service_schema, session_state, missing_fields)\n",
        "    reply = generate_allam(prompt, max_new_tokens=192)\n",
        "    return reply\n"
      ],
      "metadata": {
        "id": "o5jnTuiIHit_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P1QT0yhZUUIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ٍRouting Script"
      ],
      "metadata": {
        "id": "WgRj9nrfUUlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_router(mode, payload):\n",
        "    if mode == \"classify_intent\":\n",
        "        user_message = payload[\"user_message\"]\n",
        "        return classify_intent(user_message)\n",
        "\n",
        "    if mode == \"extract_slots\":\n",
        "        user_message = payload[\"user_message\"]\n",
        "        expected_slots = payload[\"expected_slots\"]\n",
        "        return extract_slots(user_message, expected_slots)\n",
        "\n",
        "    if mode == \"rag_answer\":\n",
        "        user_message = payload[\"user_message\"]\n",
        "        context_chunks = payload.get(\"context_chunks\", [])\n",
        "        return {\"answer\": rag_answer(user_message, context_chunks)}\n",
        "\n",
        "    if mode == \"step_guide\":\n",
        "        service_schema = payload[\"service_schema\"]\n",
        "        session_state = payload[\"session_state\"]\n",
        "        missing_fields = payload.get(\"missing_fields\", [])\n",
        "        return {\"reply\": step_guide(service_schema, session_state, missing_fields)}\n",
        "\n",
        "    return {\"error\": \"unknown_mode\"}"
      ],
      "metadata": {
        "id": "yi92d5l6Hm6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = ai_router(\"classify_intent\", {\n",
        "    \"user_message\": \"حاولت احدث رقم الجوال وما قدرت ادخل على ابشر ابدا\"\n",
        "})\n",
        "res\n"
      ],
      "metadata": {
        "id": "XfUyLUPfHngw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slots_res = ai_router(\"extract_slots\", {\n",
        "    \"user_message\": \"انا مسجل في ابشر برقم هوية 1XXXXXXXXX وابغى اغير جوالي القديم\",\n",
        "    \"expected_slots\": [\"national_id\", \"mobile_number\", \"service_type\"]\n",
        "})\n",
        "slots_res\n"
      ],
      "metadata": {
        "id": "uEZLtLoNHrXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xCpXjyaHs0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}